<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.49" />
  <meta name="author" content="Abhishek Sharma">

  
  
  
  
    
  
  <meta name="description" content="1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.">

  
  <link rel="alternate" hreflang="en-us" href="/tutorial/guardrails-ai/guardrail_serve/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href=//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono>
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-127723122-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Abhishek Sharma">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Abhishek Sharma">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/tutorial/guardrails-ai/guardrail_serve/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@abhishek9sh">
  <meta property="twitter:creator" content="@abhishek9sh">
  
  <meta property="og:site_name" content="Abhishek Sharma">
  <meta property="og:url" content="/tutorial/guardrails-ai/guardrail_serve/">
  <meta property="og:title" content="Part 2: Deploying a Guarded Endpoint Using FastAPI | Abhishek Sharma">
  <meta property="og:description" content="1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.">
  
  
    
  <meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2025-02-23T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2025-02-23T00:00:00&#43;08:00">
  

  

  

  <title>Part 2: Deploying a Guarded Endpoint Using FastAPI | Abhishek Sharma</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Abhishek Sharma</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#opensourcesoftware">
            
            <span>Software</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#certifications">
            
            <span>Certifications</span>
            
          </a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true">
            
            <span>Tutorials</span>
            
            <span class="caret"></span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="/tutorial/guardrails-ai">
                
                <span>Guardrails-AI</span>
                
              </a>
            </li>
            
          </ul>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorial/guardrails-ai/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/tutorial/guardrails-ai/guardrail_intro/">Guardrails-AI</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/tutorial/guardrails-ai/guardrail_intro/">01_intro_to_guardrails</a>
      </li>
      
      <li class="active">
        <a href="/tutorial/guardrails-ai/guardrail_serve/">02_serving</a>
      </li>
      
      <li >
        <a href="/tutorial/guardrails-ai/guardrail_validator/">03_custom_validator</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#1-set-up-environment">1. Set up environment</a>
<ul>
<li><a href="#1-1-install-a-virtual-env-with-all-dependencies">1.1 Install a Virtual env with all dependencies</a>
<ul>
<li><a href="#1-1-1-uv-based-environment-creation">1.1.1 UV Based Environment Creation</a></li>
<li><a href="#1-1-2-pip-based-environment-creation">1.1.2 PIP Based Environment Creation</a></li>
</ul></li>
<li><a href="#1-2-activate-the-kernel">1.2 Activate the Kernel</a></li>
</ul></li>
<li><a href="#2-simple-llm-chat-completions-endpoint">2. Simple LLM Chat_Completions Endpoint</a>
<ul>
<li><a href="#2-1-set-up-your-llm-provider-and-authentication-token">2.1 Set up your LLM Provider and Authentication token</a></li>
<li><a href="#2-2-make-an-simple-chat-completions-endpoint">2.2 Make an simple chat_completions endpoint</a></li>
</ul></li>
<li><a href="#3-guarded-llm-chat-completions-endpoint">3. Guarded LLM Chat_Completions Endpoint</a>
<ul>
<li><a href="#3-1-install-guard-from-guardrails-hub">3.1 Install Guard from Guardrails HUB</a>
<ul>
<li><a href="#3-1-1-configure-hub-token">3.1.1 Configure Hub Token</a></li>
<li><a href="#3-1-2-install-guardrail-from-hub">3.1.2 Install Guardrail From Hub</a></li>
</ul></li>
<li><a href="#3-2-call-llm-with-guardrails">3.2 Call LLM with Guardrails</a>
<ul>
<li><a href="#3-2-1-initialize-guardrail-object">3.2.1 Initialize Guardrail Object</a></li>
<li><a href="#3-2-2-expose-an-guarded-chat-completions-endpoint">3.2.2 Expose an Guarded chat_completions endpoint</a></li>
</ul></li>
</ul></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">
      <div id="search-hits">
        
      </div>
      <article class="article" itemscope itemtype="http://schema.org/Article">

        


        <div class="docs-article-container">
          <h1 itemprop="name">Part 2: Deploying a Guarded Endpoint Using FastAPI</h1>

          <div class="article-style" itemprop="articleBody">
            

<h1 id="1-set-up-environment">1. Set up environment</h1>

<h2 id="1-1-install-a-virtual-env-with-all-dependencies">1.1 Install a Virtual env with all dependencies</h2>

<h3 id="1-1-1-uv-based-environment-creation">1.1.1 UV Based Environment Creation</h3>

<ul>
<li>Running below cell  requires uv to be installed on your machine.</li>
<li>You can install from <a href="https://docs.astral.sh/uv/pip/environments/" target="_blank">https://docs.astral.sh/uv/pip/environments/</a></li>
<li>If you dont want UV please use pip based install</li>
</ul>

<pre><code class="language-bash">%%bash
uv venv guarded_llm_env
source guarded_llm_env/bin/activate
uv pip install ipykernel nbconvert
uv pip install guardrails-ai==0.6.3 --prerelease allow
uv pip install fastapi uvicorn nest-asyncio
python -m ipykernel install --user --name=guarded_llm_env

</code></pre>

<h3 id="1-1-2-pip-based-environment-creation">1.1.2 PIP Based Environment Creation</h3>

<ul>
<li>Uncomment below a dn run if you do want to not use above uv base install</li>
</ul>

<pre><code class="language-python"># %%bash
# python -m pip install --user virtualenv
# python -m virtualenv guarded_llm_env
# source guarded_llm_env/bin/activate
# python -m pip install ipykernel nbconvert
# python -m pip install guardrails-ai==0.6.3 
# python -m pip install fastapi uvicorn nest-asyncio
# python -m ipykernel install --user --name=guarded_llm_env
</code></pre>

<h2 id="1-2-activate-the-kernel">1.2 Activate the Kernel</h2>

<ul>
<li>refresh the browser</li>
<li>activate the <strong>guarded_llm_env</strong> kernel</li>
</ul>

<h1 id="2-simple-llm-chat-completions-endpoint">2. Simple LLM Chat_Completions Endpoint</h1>

<h2 id="2-1-set-up-your-llm-provider-and-authentication-token">2.1 Set up your LLM Provider and Authentication token</h2>

<pre><code class="language-python">import os
os.environ[&quot;LLM_API_TOKEN&quot;] = &quot;sk-123&quot;

</code></pre>

<pre><code class="language-python">import os
LLM_PROVIDER_BASE=&quot;https://api.openai.com/v1&quot;
LLM_API_TOKEN=os.environ[&quot;LLM_API_TOKEN&quot;] 
</code></pre>

<pre><code class="language-python">from typing import List, Optional
from pydantic import BaseModel

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionsReq(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
</code></pre>

<h2 id="2-2-make-an-simple-chat-completions-endpoint">2.2 Make an simple chat_completions endpoint</h2>

<ul>
<li>I am using litellm completion method here</li>
<li>Reference : <a href="https://docs.litellm.ai/docs/completion/input" target="_blank">https://docs.litellm.ai/docs/completion/input</a></li>
</ul>

<pre><code class="language-python">import litellm
from typing import Dict, Any

def call_llm(provider_base, provider_key, *args, **kwargs) -&gt; str:
    &quot;&quot;&quot;Calls an LLM using litellm.completion.&quot;&quot;&quot;
    #some bug in litellm version
    if &quot;msg_history&quot; in kwargs:
        kwargs.pop(&quot;msg_history&quot;)
        
    response = litellm.completion(
        api_base=provider_base,
        api_key=provider_key,
        **kwargs
    )
    if &quot;stream&quot; in kwargs and kwargs[&quot;stream&quot;]:
        for resp in response:
            if resp.choices[0].delta.content:  # Some responses may not have content
                chunk = resp.choices[0].delta.content
                #print(chunk, end=&quot;&quot;, flush=True)  # Print in real-time
                yield chunk
    else:
         yield response['choices'][0]['message']['content']
</code></pre>

<pre><code class="language-python">import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app = fastapi.FastAPI()

@app.post(&quot;/chat_completions&quot;)
def chatcompletion(chat_req: ChatCompletionsReq):
    chat_req_dict = chat_req.dict()
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict)
            for result in completion_outcome:
                yield str(result) + &quot; &quot;

        return StreamingResponse(stream_responses(), media_type=&quot;text/event-stream&quot;)
    else:
        completion_outcome = completion_gg(chat_req)
        if error:
            return &quot; &quot;.join(completion_outcome)
        else:
            res = &quot; &quot;.join([v for v in completion_outcome])
            return res

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=9000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

</code></pre>

<pre><code class="language-bash">%%bash
curl -X 'POST' \
  'http://localhost:9000/chat_completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
     &quot;messages&quot;:[
         {&quot;role&quot;: &quot;user&quot;, 
          &quot;content&quot;: &quot;Are python developers dumb idiotic and should they use rust&quot;}
     ],
    &quot;stream&quot;:true,
    &quot;max_tokens&quot;:50,
    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;
}'
</code></pre>

<h1 id="3-guarded-llm-chat-completions-endpoint">3. Guarded LLM Chat_Completions Endpoint</h1>

<h2 id="3-1-install-guard-from-guardrails-hub">3.1 Install Guard from Guardrails HUB</h2>

<ul>
<li>Go to <a href="https://hub.guardrailsai.com/" target="_blank">https://hub.guardrailsai.com/</a></li>
<li>Get Your token and configure it locally</li>
<li>Install your required guard</li>
</ul>

<h3 id="3-1-1-configure-hub-token">3.1.1 Configure Hub Token</h3>

<pre><code class="language-python">import os
os.environ[&quot;GR_TOKEN&quot;]=&quot;&quot;
</code></pre>

<pre><code class="language-bash">%%bash
source guarded_llm_env/bin/activate
guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN
</code></pre>

<h3 id="3-1-2-install-guardrail-from-hub">3.1.2 Install Guardrail From Hub</h3>

<pre><code class="language-bash">%%bash
source guarded_llm_env/bin/activate &amp;&amp; guardrails hub install hub://guardrails/profanity_free
</code></pre>

<h2 id="3-2-call-llm-with-guardrails">3.2 Call LLM with Guardrails</h2>

<h3 id="3-2-1-initialize-guardrail-object">3.2.1 Initialize Guardrail Object</h3>

<pre><code class="language-python">import guardrails as gd
from guardrails.hub import ProfanityFree
from guardrails import OnFailAction
profanity_guard =  gd.Guard(name=&quot;Profanity&quot;).use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)
</code></pre>

<pre><code class="language-python">## Add a New Schema to Support Guards
class ChatCompletionsReqGuarded(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
    guard_to_apply: Optional[str] = None


available_guards ={&quot;Profanity&quot;:profanity_guard}
</code></pre>

<h3 id="3-2-2-expose-an-guarded-chat-completions-endpoint">3.2.2 Expose an Guarded chat_completions endpoint</h3>

<pre><code class="language-python">def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -&gt; str:
    &quot;&quot;&quot;Calls an LLM with Profanity Guard&quot;&quot;&quot;
    if guard_to_apply:

        #Validate Input Only
        try:
            for msg in chat_request[&quot;messages&quot;]:
                guard_to_apply.validate(msg[&quot;content&quot;])
        except Exception as e:
            error_str = &quot;INPUT_GUARD_FAILED::&quot; + str(e)
            yield error_str
            return
        
        try:
            #FIX ME SOME BUG HERE
            llm_output_gen = profanity_guard(call_llm,
                                            provider_base=LLM_PROVIDER_BASE, 
                                            provider_key=LLM_API_TOKEN, 
                                            **chat_request)
            for validation_outcome in llm_output_gen:
                if validation_outcome.validation_passed==True:
                    yield validation_outcome.validated_output
        except Exception as e:
            error_str = &quot;OUTPUT_GUARD_FAILED::&quot; + str(e)
            print(error_str)
            yield error_str
            #return 
            
            
    else:
        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,
                                   provider_key=LLM_API_TOKEN,
                                   **user_chat_request):
            yield chunk_resp

</code></pre>

<pre><code class="language-python">import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app_guarded = fastapi.FastAPI()

@app_guarded.post(&quot;/ChatCompletionsReqGuarded&quot;)
def chatcompletion(chat_req: ChatCompletionsReqGuarded):
    chat_req_dict = chat_req.dict()
    guard_to_apply = available_guards[chat_req.guard_to_apply]
    chat_req_dict.pop(&quot;guard_to_apply&quot;)
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
            for result in completion_outcome:
                yield str(result) + &quot; &quot;

        return StreamingResponse(stream_responses(), media_type=&quot;text/event-stream&quot;)
    else:
        completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
        return completion_outcome#FIX THIS

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app_guarded, 
                host=&quot;0.0.0.0&quot;, 
                port=8000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

</code></pre>

<pre><code class="language-bash">%%bash
curl -X 'POST' \
  'http://localhost:8000/ChatCompletionsReqGuarded' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
     &quot;messages&quot;:[
         {&quot;role&quot;: &quot;user&quot;, 
          &quot;content&quot;: &quot;Are python developers dumb idiotic and should they use rust &quot;}
     ],
    &quot;stream&quot;:true,
    &quot;max_tokens&quot;:50,
    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
    &quot;guard_to_apply&quot;:&quot;Profanity&quot;

}'
</code></pre>

<pre><code class="language-bash">%%bash
curl -X 'POST' \
  'http://localhost:8000/ChatCompletionsReqGuarded' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
     &quot;messages&quot;:[
         {&quot;role&quot;: &quot;user&quot;, 
          &quot;content&quot;: &quot;Complete the below sentence. he is in id**t &quot;}
     ],
    &quot;stream&quot;:true,
    &quot;max_tokens&quot;:50,
    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
    &quot;guard_to_apply&quot;:&quot;Profanity&quot;

}'
</code></pre>

<pre><code class="language-python">
</code></pre>

          </div>

          

        </div>

        <div class="body-footer">
          Last updated on Feb 23, 2025
        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>


