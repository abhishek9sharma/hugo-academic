<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Abhishek Sharma on Abhishek Sharma</title>
    <link>/</link>
    <description>Recent content in Abhishek Sharma on Abhishek Sharma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 27 Sep 2018 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Part 1: Introduction to Guardrails-AI Python Library</title>
      <link>/tutorial/guardrails-ai/guardrail_intro/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0800</pubDate>
      
      <guid>/tutorial/guardrails-ai/guardrail_intro/</guid>
      <description>

&lt;h1 id=&#34;1-set-up-environment&#34;&gt;1. Set up environment&lt;/h1&gt;

&lt;h2 id=&#34;1-1-install-a-virtual-env-with-all-dependencies&#34;&gt;1.1 Install a Virtual env with all dependencies&lt;/h2&gt;

&lt;h3 id=&#34;1-1-1-uv-based-environment-creation&#34;&gt;1.1.1 UV Based Environment Creation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Running below cell  requires uv to be installed on your machine.&lt;/li&gt;
&lt;li&gt;You can install from &lt;a href=&#34;https://docs.astral.sh/uv/pip/environments/&#34; target=&#34;_blank&#34;&gt;https://docs.astral.sh/uv/pip/environments/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you dont want UV please use pip based install&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
uv venv guarded_llm_env
source guarded_llm_env/bin/activate
uv pip install ipykernel nbconvert
uv pip install guardrails-ai==0.6.3 --prerelease allow
uv pip install fastapi uvicorn nest-asyncio
python -m ipykernel install --user --name=guarded_llm_env

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-1-2-pip-based-environment-creation&#34;&gt;1.1.2 PIP Based Environment Creation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Uncomment below a dn run if you do want to not use above uv base install&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# %%bash
# python -m pip install --user virtualenv
# python -m virtualenv guarded_llm_env
# source guarded_llm_env/bin/activate
# python -m pip install ipykernel nbconvert
# python -m pip install guardrails-ai==0.6.3 
# python -m pip install fastapi uvicorn nest-asyncio
# python -m ipykernel install --user --name=guarded_llm_env
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;1-2-activate-the-kernel&#34;&gt;1.2 Activate the Kernel&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;refresh the browser&lt;/li&gt;
&lt;li&gt;activate the &lt;strong&gt;guarded_llm_env&lt;/strong&gt; kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-simple-llm-chat-completions-endpoint&#34;&gt;2. Simple LLM Chat_Completions Endpoint&lt;/h1&gt;

&lt;h2 id=&#34;2-1-set-up-your-llm-provider-and-authentication-token&#34;&gt;2.1 Set up your LLM Provider and Authentication token&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
os.environ[&amp;quot;LLM_API_TOKEN&amp;quot;] = &amp;quot;sk-123&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
LLM_PROVIDER_BASE=&amp;quot;https://api.openai.com/v1&amp;quot;
LLM_API_TOKEN=os.environ[&amp;quot;LLM_API_TOKEN&amp;quot;] 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List, Optional
from pydantic import BaseModel

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionsReq(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-make-an-simple-chat-completions-endpoint&#34;&gt;2.2 Make an simple chat_completions endpoint&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I am using litellm completion method here&lt;/li&gt;
&lt;li&gt;Reference : &lt;a href=&#34;https://docs.litellm.ai/docs/completion/input&#34; target=&#34;_blank&#34;&gt;https://docs.litellm.ai/docs/completion/input&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import litellm
from typing import Dict, Any

def call_llm(provider_base, provider_key, *args, **kwargs) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Calls an LLM using litellm.completion.&amp;quot;&amp;quot;&amp;quot;
    #some bug in litellm version
    if &amp;quot;msg_history&amp;quot; in kwargs:
        kwargs.pop(&amp;quot;msg_history&amp;quot;)
        
    response = litellm.completion(
        api_base=provider_base,
        api_key=provider_key,
        **kwargs
    )
    if &amp;quot;stream&amp;quot; in kwargs and kwargs[&amp;quot;stream&amp;quot;]:
        for resp in response:
            if resp.choices[0].delta.content:  # Some responses may not have content
                chunk = resp.choices[0].delta.content
                #print(chunk, end=&amp;quot;&amp;quot;, flush=True)  # Print in real-time
                yield chunk
    else:
         yield response[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app = fastapi.FastAPI()

@app.post(&amp;quot;/chat_completions&amp;quot;)
def chatcompletion(chat_req: ChatCompletionsReq):
    chat_req_dict = chat_req.dict()
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict)
            for result in completion_outcome:
                yield str(result) + &amp;quot; &amp;quot;

        return StreamingResponse(stream_responses(), media_type=&amp;quot;text/event-stream&amp;quot;)
    else:
        completion_outcome = completion_gg(chat_req)
        if error:
            return &amp;quot; &amp;quot;.join(completion_outcome)
        else:
            res = &amp;quot; &amp;quot;.join([v for v in completion_outcome])
            return res

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app, host=&amp;quot;0.0.0.0&amp;quot;, port=9000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:9000/chat_completions&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Are python developers dumb idiotic and should they use rust&amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-guarded-llm-chat-completions-endpoint&#34;&gt;3. Guarded LLM Chat_Completions Endpoint&lt;/h1&gt;

&lt;h2 id=&#34;3-1-install-guard-from-guardrails-hub&#34;&gt;3.1 Install Guard from Guardrails HUB&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://hub.guardrailsai.com/&#34; target=&#34;_blank&#34;&gt;https://hub.guardrailsai.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Get Your token and configure it locally&lt;/li&gt;
&lt;li&gt;Install your required guard&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-1-1-configure-hub-token&#34;&gt;3.1.1 Configure Hub Token&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
os.environ[&amp;quot;GR_TOKEN&amp;quot;]=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
source guarded_llm_env/bin/activate
guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-2-install-guardrail-from-hub&#34;&gt;3.1.2 Install Guardrail From Hub&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
source guarded_llm_env/bin/activate &amp;amp;&amp;amp; guardrails hub install hub://guardrails/profanity_free
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-call-llm-with-guardrails&#34;&gt;3.2 Call LLM with Guardrails&lt;/h2&gt;

&lt;h3 id=&#34;3-2-1-initialize-guardrail-object&#34;&gt;3.2.1 Initialize Guardrail Object&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guardrails as gd
from guardrails.hub import ProfanityFree
from guardrails import OnFailAction
profanity_guard =  gd.Guard(name=&amp;quot;Profanity&amp;quot;).use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Add a New Schema to Support Guards
class ChatCompletionsReqGuarded(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
    guard_to_apply: Optional[str] = None


available_guards ={&amp;quot;Profanity&amp;quot;:profanity_guard}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-2-expose-an-guarded-chat-completions-endpoint&#34;&gt;3.2.2 Expose an Guarded chat_completions endpoint&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Calls an LLM with Profanity Guard&amp;quot;&amp;quot;&amp;quot;
    if guard_to_apply:

        #Validate Input Only
        try:
            for msg in chat_request[&amp;quot;messages&amp;quot;]:
                guard_to_apply.validate(msg[&amp;quot;content&amp;quot;])
        except Exception as e:
            error_str = &amp;quot;INPUT_GUARD_FAILED::&amp;quot; + str(e)
            yield error_str
            return
        
        try:
            #FIX ME SOME BUG HERE
            llm_output_gen = profanity_guard(call_llm,
                                            provider_base=LLM_PROVIDER_BASE, 
                                            provider_key=LLM_API_TOKEN, 
                                            **chat_request)
            for validation_outcome in llm_output_gen:
                if validation_outcome.validation_passed==True:
                    yield validation_outcome.validated_output
        except Exception as e:
            error_str = &amp;quot;OUTPUT_GUARD_FAILED::&amp;quot; + str(e)
            print(error_str)
            yield error_str
            #return 
            
            
    else:
        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,
                                   provider_key=LLM_API_TOKEN,
                                   **user_chat_request):
            yield chunk_resp

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app_guarded = fastapi.FastAPI()

@app_guarded.post(&amp;quot;/ChatCompletionsReqGuarded&amp;quot;)
def chatcompletion(chat_req: ChatCompletionsReqGuarded):
    chat_req_dict = chat_req.dict()
    guard_to_apply = available_guards[chat_req.guard_to_apply]
    chat_req_dict.pop(&amp;quot;guard_to_apply&amp;quot;)
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
            for result in completion_outcome:
                yield str(result) + &amp;quot; &amp;quot;

        return StreamingResponse(stream_responses(), media_type=&amp;quot;text/event-stream&amp;quot;)
    else:
        completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
        return completion_outcome#FIX THIS

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app_guarded, 
                host=&amp;quot;0.0.0.0&amp;quot;, 
                port=8000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:8000/ChatCompletionsReqGuarded&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Are python developers dumb idiotic and should they use rust &amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;,
    &amp;quot;guard_to_apply&amp;quot;:&amp;quot;Profanity&amp;quot;

}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:8000/ChatCompletionsReqGuarded&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Complete the below sentence. he is in id**t &amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;,
    &amp;quot;guard_to_apply&amp;quot;:&amp;quot;Profanity&amp;quot;

}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Part 2: Deploying a Guarded Endpoint Using FastAPI</title>
      <link>/tutorial/guardrails-ai/guardrail_serve/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0800</pubDate>
      
      <guid>/tutorial/guardrails-ai/guardrail_serve/</guid>
      <description>

&lt;h1 id=&#34;1-set-up-environment&#34;&gt;1. Set up environment&lt;/h1&gt;

&lt;h2 id=&#34;1-1-install-a-virtual-env-with-all-dependencies&#34;&gt;1.1 Install a Virtual env with all dependencies&lt;/h2&gt;

&lt;h3 id=&#34;1-1-1-uv-based-environment-creation&#34;&gt;1.1.1 UV Based Environment Creation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Running below cell  requires uv to be installed on your machine.&lt;/li&gt;
&lt;li&gt;You can install from &lt;a href=&#34;https://docs.astral.sh/uv/pip/environments/&#34; target=&#34;_blank&#34;&gt;https://docs.astral.sh/uv/pip/environments/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you dont want UV please use pip based install&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
uv venv guarded_llm_env
source guarded_llm_env/bin/activate
uv pip install ipykernel nbconvert
uv pip install guardrails-ai==0.6.3 --prerelease allow
uv pip install fastapi uvicorn nest-asyncio
python -m ipykernel install --user --name=guarded_llm_env

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-1-2-pip-based-environment-creation&#34;&gt;1.1.2 PIP Based Environment Creation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Uncomment below a dn run if you do want to not use above uv base install&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# %%bash
# python -m pip install --user virtualenv
# python -m virtualenv guarded_llm_env
# source guarded_llm_env/bin/activate
# python -m pip install ipykernel nbconvert
# python -m pip install guardrails-ai==0.6.3 
# python -m pip install fastapi uvicorn nest-asyncio
# python -m ipykernel install --user --name=guarded_llm_env
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;1-2-activate-the-kernel&#34;&gt;1.2 Activate the Kernel&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;refresh the browser&lt;/li&gt;
&lt;li&gt;activate the &lt;strong&gt;guarded_llm_env&lt;/strong&gt; kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-simple-llm-chat-completions-endpoint&#34;&gt;2. Simple LLM Chat_Completions Endpoint&lt;/h1&gt;

&lt;h2 id=&#34;2-1-set-up-your-llm-provider-and-authentication-token&#34;&gt;2.1 Set up your LLM Provider and Authentication token&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
os.environ[&amp;quot;LLM_API_TOKEN&amp;quot;] = &amp;quot;sk-123&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
LLM_PROVIDER_BASE=&amp;quot;https://api.openai.com/v1&amp;quot;
LLM_API_TOKEN=os.environ[&amp;quot;LLM_API_TOKEN&amp;quot;] 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import List, Optional
from pydantic import BaseModel

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionsReq(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-2-make-an-simple-chat-completions-endpoint&#34;&gt;2.2 Make an simple chat_completions endpoint&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I am using litellm completion method here&lt;/li&gt;
&lt;li&gt;Reference : &lt;a href=&#34;https://docs.litellm.ai/docs/completion/input&#34; target=&#34;_blank&#34;&gt;https://docs.litellm.ai/docs/completion/input&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import litellm
from typing import Dict, Any

def call_llm(provider_base, provider_key, *args, **kwargs) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Calls an LLM using litellm.completion.&amp;quot;&amp;quot;&amp;quot;
    #some bug in litellm version
    if &amp;quot;msg_history&amp;quot; in kwargs:
        kwargs.pop(&amp;quot;msg_history&amp;quot;)
        
    response = litellm.completion(
        api_base=provider_base,
        api_key=provider_key,
        **kwargs
    )
    if &amp;quot;stream&amp;quot; in kwargs and kwargs[&amp;quot;stream&amp;quot;]:
        for resp in response:
            if resp.choices[0].delta.content:  # Some responses may not have content
                chunk = resp.choices[0].delta.content
                #print(chunk, end=&amp;quot;&amp;quot;, flush=True)  # Print in real-time
                yield chunk
    else:
         yield response[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app = fastapi.FastAPI()

@app.post(&amp;quot;/chat_completions&amp;quot;)
def chatcompletion(chat_req: ChatCompletionsReq):
    chat_req_dict = chat_req.dict()
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict)
            for result in completion_outcome:
                yield str(result) + &amp;quot; &amp;quot;

        return StreamingResponse(stream_responses(), media_type=&amp;quot;text/event-stream&amp;quot;)
    else:
        completion_outcome = completion_gg(chat_req)
        if error:
            return &amp;quot; &amp;quot;.join(completion_outcome)
        else:
            res = &amp;quot; &amp;quot;.join([v for v in completion_outcome])
            return res

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app, host=&amp;quot;0.0.0.0&amp;quot;, port=9000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:9000/chat_completions&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Are python developers dumb idiotic and should they use rust&amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-guarded-llm-chat-completions-endpoint&#34;&gt;3. Guarded LLM Chat_Completions Endpoint&lt;/h1&gt;

&lt;h2 id=&#34;3-1-install-guard-from-guardrails-hub&#34;&gt;3.1 Install Guard from Guardrails HUB&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://hub.guardrailsai.com/&#34; target=&#34;_blank&#34;&gt;https://hub.guardrailsai.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Get Your token and configure it locally&lt;/li&gt;
&lt;li&gt;Install your required guard&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-1-1-configure-hub-token&#34;&gt;3.1.1 Configure Hub Token&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
os.environ[&amp;quot;GR_TOKEN&amp;quot;]=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
source guarded_llm_env/bin/activate
guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-1-2-install-guardrail-from-hub&#34;&gt;3.1.2 Install Guardrail From Hub&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
source guarded_llm_env/bin/activate &amp;amp;&amp;amp; guardrails hub install hub://guardrails/profanity_free
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-2-call-llm-with-guardrails&#34;&gt;3.2 Call LLM with Guardrails&lt;/h2&gt;

&lt;h3 id=&#34;3-2-1-initialize-guardrail-object&#34;&gt;3.2.1 Initialize Guardrail Object&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import guardrails as gd
from guardrails.hub import ProfanityFree
from guardrails import OnFailAction
profanity_guard =  gd.Guard(name=&amp;quot;Profanity&amp;quot;).use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Add a New Schema to Support Guards
class ChatCompletionsReqGuarded(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 100
    stream: Optional[bool] = True
    guard_to_apply: Optional[str] = None


available_guards ={&amp;quot;Profanity&amp;quot;:profanity_guard}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-2-expose-an-guarded-chat-completions-endpoint&#34;&gt;3.2.2 Expose an Guarded chat_completions endpoint&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Calls an LLM with Profanity Guard&amp;quot;&amp;quot;&amp;quot;
    if guard_to_apply:

        #Validate Input Only
        try:
            for msg in chat_request[&amp;quot;messages&amp;quot;]:
                guard_to_apply.validate(msg[&amp;quot;content&amp;quot;])
        except Exception as e:
            error_str = &amp;quot;INPUT_GUARD_FAILED::&amp;quot; + str(e)
            yield error_str
            return
        
        try:
            #FIX ME SOME BUG HERE
            llm_output_gen = profanity_guard(call_llm,
                                            provider_base=LLM_PROVIDER_BASE, 
                                            provider_key=LLM_API_TOKEN, 
                                            **chat_request)
            for validation_outcome in llm_output_gen:
                if validation_outcome.validation_passed==True:
                    yield validation_outcome.validated_output
        except Exception as e:
            error_str = &amp;quot;OUTPUT_GUARD_FAILED::&amp;quot; + str(e)
            print(error_str)
            yield error_str
            #return 
            
            
    else:
        for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE,
                                   provider_key=LLM_API_TOKEN,
                                   **user_chat_request):
            yield chunk_resp

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nest_asyncio
import fastapi
import uvicorn
import threading
from starlette.responses import StreamingResponse

app_guarded = fastapi.FastAPI()

@app_guarded.post(&amp;quot;/ChatCompletionsReqGuarded&amp;quot;)
def chatcompletion(chat_req: ChatCompletionsReqGuarded):
    chat_req_dict = chat_req.dict()
    guard_to_apply = available_guards[chat_req.guard_to_apply]
    chat_req_dict.pop(&amp;quot;guard_to_apply&amp;quot;)
    if chat_req.stream:
        def stream_responses():
            completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
            for result in completion_outcome:
                yield str(result) + &amp;quot; &amp;quot;

        return StreamingResponse(stream_responses(), media_type=&amp;quot;text/event-stream&amp;quot;)
    else:
        completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, 
                                                  provider_key=LLM_API_TOKEN, 
                                                  chat_request=chat_req_dict, 
                                                  guard_to_apply=guard_to_apply)
        return completion_outcome#FIX THIS

# Function to run the server in a background thread
def run():
    nest_asyncio.apply()
    uvicorn.run(app_guarded, 
                host=&amp;quot;0.0.0.0&amp;quot;, 
                port=8000)

# Start the FastAPI server in a separate thread
server_thread = threading.Thread(target=run, daemon=True)
server_thread.start()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:8000/ChatCompletionsReqGuarded&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Are python developers dumb idiotic and should they use rust &amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;,
    &amp;quot;guard_to_apply&amp;quot;:&amp;quot;Profanity&amp;quot;

}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;%%bash
curl -X &#39;POST&#39; \
  &#39;http://localhost:8000/ChatCompletionsReqGuarded&#39; \
  -H &#39;accept: application/json&#39; \
  -H &#39;Content-Type: application/json&#39; \
  -d &#39;{
     &amp;quot;messages&amp;quot;:[
         {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, 
          &amp;quot;content&amp;quot;: &amp;quot;Complete the below sentence. he is in id**t &amp;quot;}
     ],
    &amp;quot;stream&amp;quot;:true,
    &amp;quot;max_tokens&amp;quot;:50,
    &amp;quot;model&amp;quot;: &amp;quot;gpt-3.5-turbo&amp;quot;,
    &amp;quot;guard_to_apply&amp;quot;:&amp;quot;Profanity&amp;quot;

}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>sarathi</title>
      <link>/opensource/sarathi/</link>
      <pubDate>Tue, 07 May 2024 00:00:00 +0800</pubDate>
      
      <guid>/opensource/sarathi/</guid>
      <description>&lt;p&gt;Sarathi is an  LLM-based, simple command line interface-based coding assistant. ThIt ca write detailed commit messages and documentation for a python codebases &lt;a href=&#34;https://github.com/abhishek9sharma/sarathi&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated Identification of Libraries from Vulnerability Data:Can We Do Better?</title>
      <link>/publication/xmtc/</link>
      <pubDate>Sat, 19 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>/publication/xmtc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyzing Offline Social Engagements: AnEmpirical Study of Meetup Events Related to Software Development</title>
      <link>/publication/meetup/</link>
      <pubDate>Fri, 14 Jan 2022 01:00:00 +0800</pubDate>
      
      <guid>/publication/meetup/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HERMES: Using Commit-Issue Linking to Detect Vulnerability-Fixing Commits</title>
      <link>/publication/vcur2/</link>
      <pubDate>Fri, 14 Jan 2022 00:00:00 +0800</pubDate>
      
      <guid>/publication/vcur2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>mlopsdemo</title>
      <link>/opensource/mlops/</link>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0800</pubDate>
      
      <guid>/opensource/mlops/</guid>
      <description>&lt;p&gt;A mini Dockerized ML system with MLOPS capabilities (train/deploy). It can train/deploy models for recommendation/classification given user+stores click interaction data set and automatically deploy models to be served by a simple python web API end point. You can find the source code of the application &lt;a href=&#34;https://github.com/abhishek9sharma/mlopsdemo&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Post2Vec: Learning Distributed Representations of Stack Overflow Posts</title>
      <link>/publication/post2vec/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0800</pubDate>
      
      <guid>/publication/post2vec/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Out of sight, out of mind? How vulnerable dependencies affect open-source projects</title>
      <link>/publication/outofsight/</link>
      <pubDate>Wed, 21 Apr 2021 00:00:00 +0800</pubDate>
      
      <guid>/publication/outofsight/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Machine Learning Approach for Vulnerability Curation</title>
      <link>/publication/vulncur/</link>
      <pubDate>Fri, 26 Jun 2020 00:00:00 +0800</pubDate>
      
      <guid>/publication/vulncur/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SIEVE: Helping Developers Sift Wheat from Chaff via Cross-Platform Analysis</title>
      <link>/publication/sieve/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>/publication/sieve/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recommending Who to Follow in the Software Engineering Twitter Space</title>
      <link>/publication/software-experts/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0800</pubDate>
      
      <guid>/publication/software-experts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Part 3: Adding a Custom Validator to Guardrails-AI</title>
      <link>/tutorial/guardrails-ai/guardrail_validator/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0800</pubDate>
      
      <guid>/tutorial/guardrails-ai/guardrail_validator/</guid>
      <description></description>
    </item>
    
    <item>
      <title>APIBot: Question Answering Bot for API documentation</title>
      <link>/publication/apibot/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0800</pubDate>
      
      <guid>/publication/apibot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personality and Project Success: Insights from a Large-Scale Study with Professionals</title>
      <link>/publication/personality/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0800</pubDate>
      
      <guid>/publication/personality/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
