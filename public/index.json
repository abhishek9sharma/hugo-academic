[{"authors":null,"categories":null,"content":" 1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.1.2 PIP Based Environment Creation  Uncomment below a dn run if you do want to not use above uv base install  # %%bash # python -m pip install --user virtualenv # python -m virtualenv guarded_llm_env # source guarded_llm_env/bin/activate # python -m pip install ipykernel nbconvert # python -m pip install guardrails-ai==0.6.3 # python -m pip install fastapi uvicorn nest-asyncio # python -m ipykernel install --user --name=guarded_llm_env  1.2 Activate the Kernel  refresh the browser activate the guarded_llm_env kernel  2. Simple LLM Chat_Completions Endpoint 2.1 Set up your LLM Provider and Authentication token import os os.environ[\u0026quot;LLM_API_TOKEN\u0026quot;] = \u0026quot;sk-123\u0026quot;  import os LLM_PROVIDER_BASE=\u0026quot;https://api.openai.com/v1\u0026quot; LLM_API_TOKEN=os.environ[\u0026quot;LLM_API_TOKEN\u0026quot;]  from typing import List, Optional from pydantic import BaseModel class Message(BaseModel): role: str content: str class ChatCompletionsReq(BaseModel): model: str messages: List[Message] max_tokens: Optional[int] = 100 stream: Optional[bool] = True  2.2 Make an simple chat_completions endpoint  I am using litellm completion method here Reference : https://docs.litellm.ai/docs/completion/input  import litellm from typing import Dict, Any def call_llm(provider_base, provider_key, *args, **kwargs) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Calls an LLM using litellm.completion.\u0026quot;\u0026quot;\u0026quot; #some bug in litellm version if \u0026quot;msg_history\u0026quot; in kwargs: kwargs.pop(\u0026quot;msg_history\u0026quot;) response = litellm.completion( api_base=provider_base, api_key=provider_key, **kwargs ) if \u0026quot;stream\u0026quot; in kwargs and kwargs[\u0026quot;stream\u0026quot;]: for resp in response: if resp.choices[0].delta.content: # Some responses may not have content chunk = resp.choices[0].delta.content #print(chunk, end=\u0026quot;\u0026quot;, flush=True) # Print in real-time yield chunk else: yield response['choices'][0]['message']['content']  import nest_asyncio import fastapi import uvicorn import threading from starlette.responses import StreamingResponse app = fastapi.FastAPI() @app.post(\u0026quot;/chat_completions\u0026quot;) def chatcompletion(chat_req: ChatCompletionsReq): chat_req_dict = chat_req.dict() if chat_req.stream: def stream_responses(): completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict) for result in completion_outcome: yield str(result) + \u0026quot; \u0026quot; return StreamingResponse(stream_responses(), media_type=\u0026quot;text/event-stream\u0026quot;) else: completion_outcome = completion_gg(chat_req) if error: return \u0026quot; \u0026quot;.join(completion_outcome) else: res = \u0026quot; \u0026quot;.join([v for v in completion_outcome]) return res # Function to run the server in a background thread def run(): nest_asyncio.apply() uvicorn.run(app, host=\u0026quot;0.0.0.0\u0026quot;, port=9000) # Start the FastAPI server in a separate thread server_thread = threading.Thread(target=run, daemon=True) server_thread.start()  %%bash curl -X 'POST' \\ 'http://localhost:9000/chat_completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Are python developers dumb idiotic and should they use rust\u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot; }'  3. Guarded LLM Chat_Completions Endpoint 3.1 Install Guard from Guardrails HUB  Go to https://hub.guardrailsai.com/ Get Your token and configure it locally Install your required guard  3.1.1 Configure Hub Token import os os.environ[\u0026quot;GR_TOKEN\u0026quot;]=\u0026quot;\u0026quot;  %%bash source guarded_llm_env/bin/activate guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN  3.1.2 Install Guardrail From Hub %%bash source guarded_llm_env/bin/activate \u0026amp;\u0026amp; guardrails hub install hub://guardrails/profanity_free  3.2 Call LLM with Guardrails 3.2.1 Initialize Guardrail Object import guardrails as gd from guardrails.hub import ProfanityFree from guardrails import OnFailAction profanity_guard = gd.Guard(name=\u0026quot;Profanity\u0026quot;).use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)  ## Add a New Schema to Support Guards class ChatCompletionsReqGuarded(BaseModel): model: str messages: List[Message] max_tokens: Optional[int] = 100 stream: Optional[bool] = True guard_to_apply: Optional[str] = None available_guards ={\u0026quot;Profanity\u0026quot;:profanity_guard}  3.2.2 Expose an Guarded chat_completions endpoint def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Calls an LLM with Profanity Guard\u0026quot;\u0026quot;\u0026quot; if guard_to_apply: #Validate Input Only try: for msg in chat_request[\u0026quot;messages\u0026quot;]: guard_to_apply.validate(msg[\u0026quot;content\u0026quot;]) except Exception as e: error_str = \u0026quot;INPUT_GUARD_FAILED::\u0026quot; + str(e) yield error_str return try: #FIX ME SOME BUG HERE llm_output_gen = profanity_guard(call_llm, provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, **chat_request) for validation_outcome in llm_output_gen: if validation_outcome.validation_passed==True: yield validation_outcome.validated_output except Exception as e: error_str = \u0026quot;OUTPUT_GUARD_FAILED::\u0026quot; + str(e) print(error_str) yield error_str #return else: for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, **user_chat_request): yield chunk_resp  import nest_asyncio import fastapi import uvicorn import threading from starlette.responses import StreamingResponse app_guarded = fastapi.FastAPI() @app_guarded.post(\u0026quot;/ChatCompletionsReqGuarded\u0026quot;) def chatcompletion(chat_req: ChatCompletionsReqGuarded): chat_req_dict = chat_req.dict() guard_to_apply = available_guards[chat_req.guard_to_apply] chat_req_dict.pop(\u0026quot;guard_to_apply\u0026quot;) if chat_req.stream: def stream_responses(): completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, chat_request=chat_req_dict, guard_to_apply=guard_to_apply) for result in completion_outcome: yield str(result) + \u0026quot; \u0026quot; return StreamingResponse(stream_responses(), media_type=\u0026quot;text/event-stream\u0026quot;) else: completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, chat_request=chat_req_dict, guard_to_apply=guard_to_apply) return completion_outcome#FIX THIS # Function to run the server in a background thread def run(): nest_asyncio.apply() uvicorn.run(app_guarded, host=\u0026quot;0.0.0.0\u0026quot;, port=8000) # Start the FastAPI server in a separate thread server_thread = threading.Thread(target=run, daemon=True) server_thread.start()  %%bash curl -X 'POST' \\ 'http://localhost:8000/ChatCompletionsReqGuarded' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Are python developers dumb idiotic and should they use rust \u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot;, \u0026quot;guard_to_apply\u0026quot;:\u0026quot;Profanity\u0026quot; }'  %%bash curl -X 'POST' \\ 'http://localhost:8000/ChatCompletionsReqGuarded' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Complete the below sentence. he is in id**t \u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot;, \u0026quot;guard_to_apply\u0026quot;:\u0026quot;Profanity\u0026quot; }'   ","date":1740240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740240000,"objectID":"4a11c6f5204d0db0982fa63693f9dbeb","permalink":"/tutorial/guardrails-ai/guardrail_intro/","publishdate":"2025-02-23T00:00:00+08:00","relpermalink":"/tutorial/guardrails-ai/guardrail_intro/","section":"tutorial","summary":"1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.","tags":null,"title":"Part 1: Introduction to Guardrails-AI Python Library","type":"docs"},{"authors":null,"categories":null,"content":" 1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.1.2 PIP Based Environment Creation  Uncomment below a dn run if you do want to not use above uv base install  # %%bash # python -m pip install --user virtualenv # python -m virtualenv guarded_llm_env # source guarded_llm_env/bin/activate # python -m pip install ipykernel nbconvert # python -m pip install guardrails-ai==0.6.3 # python -m pip install fastapi uvicorn nest-asyncio # python -m ipykernel install --user --name=guarded_llm_env  1.2 Activate the Kernel  refresh the browser activate the guarded_llm_env kernel  2. Simple LLM Chat_Completions Endpoint 2.1 Set up your LLM Provider and Authentication token import os os.environ[\u0026quot;LLM_API_TOKEN\u0026quot;] = \u0026quot;sk-123\u0026quot;  import os LLM_PROVIDER_BASE=\u0026quot;https://api.openai.com/v1\u0026quot; LLM_API_TOKEN=os.environ[\u0026quot;LLM_API_TOKEN\u0026quot;]  from typing import List, Optional from pydantic import BaseModel class Message(BaseModel): role: str content: str class ChatCompletionsReq(BaseModel): model: str messages: List[Message] max_tokens: Optional[int] = 100 stream: Optional[bool] = True  2.2 Make an simple chat_completions endpoint  I am using litellm completion method here Reference : https://docs.litellm.ai/docs/completion/input  import litellm from typing import Dict, Any def call_llm(provider_base, provider_key, *args, **kwargs) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Calls an LLM using litellm.completion.\u0026quot;\u0026quot;\u0026quot; #some bug in litellm version if \u0026quot;msg_history\u0026quot; in kwargs: kwargs.pop(\u0026quot;msg_history\u0026quot;) response = litellm.completion( api_base=provider_base, api_key=provider_key, **kwargs ) if \u0026quot;stream\u0026quot; in kwargs and kwargs[\u0026quot;stream\u0026quot;]: for resp in response: if resp.choices[0].delta.content: # Some responses may not have content chunk = resp.choices[0].delta.content #print(chunk, end=\u0026quot;\u0026quot;, flush=True) # Print in real-time yield chunk else: yield response['choices'][0]['message']['content']  import nest_asyncio import fastapi import uvicorn import threading from starlette.responses import StreamingResponse app = fastapi.FastAPI() @app.post(\u0026quot;/chat_completions\u0026quot;) def chatcompletion(chat_req: ChatCompletionsReq): chat_req_dict = chat_req.dict() if chat_req.stream: def stream_responses(): completion_outcome = call_llm(LLM_PROVIDER_BASE, LLM_API_TOKEN, **chat_req_dict) for result in completion_outcome: yield str(result) + \u0026quot; \u0026quot; return StreamingResponse(stream_responses(), media_type=\u0026quot;text/event-stream\u0026quot;) else: completion_outcome = completion_gg(chat_req) if error: return \u0026quot; \u0026quot;.join(completion_outcome) else: res = \u0026quot; \u0026quot;.join([v for v in completion_outcome]) return res # Function to run the server in a background thread def run(): nest_asyncio.apply() uvicorn.run(app, host=\u0026quot;0.0.0.0\u0026quot;, port=9000) # Start the FastAPI server in a separate thread server_thread = threading.Thread(target=run, daemon=True) server_thread.start()  %%bash curl -X 'POST' \\ 'http://localhost:9000/chat_completions' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Are python developers dumb idiotic and should they use rust\u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot; }'  3. Guarded LLM Chat_Completions Endpoint 3.1 Install Guard from Guardrails HUB  Go to https://hub.guardrailsai.com/ Get Your token and configure it locally Install your required guard  3.1.1 Configure Hub Token import os os.environ[\u0026quot;GR_TOKEN\u0026quot;]=\u0026quot;\u0026quot;  %%bash source guarded_llm_env/bin/activate guardrails configure --disable-remote-inferencing --disable-metrics --token $GR_TOKEN  3.1.2 Install Guardrail From Hub %%bash source guarded_llm_env/bin/activate \u0026amp;\u0026amp; guardrails hub install hub://guardrails/profanity_free  3.2 Call LLM with Guardrails 3.2.1 Initialize Guardrail Object import guardrails as gd from guardrails.hub import ProfanityFree from guardrails import OnFailAction profanity_guard = gd.Guard(name=\u0026quot;Profanity\u0026quot;).use(ProfanityFree, on_fail=OnFailAction.EXCEPTION)  ## Add a New Schema to Support Guards class ChatCompletionsReqGuarded(BaseModel): model: str messages: List[Message] max_tokens: Optional[int] = 100 stream: Optional[bool] = True guard_to_apply: Optional[str] = None available_guards ={\u0026quot;Profanity\u0026quot;:profanity_guard}  3.2.2 Expose an Guarded chat_completions endpoint def call_llm_guarded(provider_base, provider_key, chat_request, guard_to_apply=None) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Calls an LLM with Profanity Guard\u0026quot;\u0026quot;\u0026quot; if guard_to_apply: #Validate Input Only try: for msg in chat_request[\u0026quot;messages\u0026quot;]: guard_to_apply.validate(msg[\u0026quot;content\u0026quot;]) except Exception as e: error_str = \u0026quot;INPUT_GUARD_FAILED::\u0026quot; + str(e) yield error_str return try: #FIX ME SOME BUG HERE llm_output_gen = profanity_guard(call_llm, provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, **chat_request) for validation_outcome in llm_output_gen: if validation_outcome.validation_passed==True: yield validation_outcome.validated_output except Exception as e: error_str = \u0026quot;OUTPUT_GUARD_FAILED::\u0026quot; + str(e) print(error_str) yield error_str #return else: for chunk_resp in call_llm(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, **user_chat_request): yield chunk_resp  import nest_asyncio import fastapi import uvicorn import threading from starlette.responses import StreamingResponse app_guarded = fastapi.FastAPI() @app_guarded.post(\u0026quot;/ChatCompletionsReqGuarded\u0026quot;) def chatcompletion(chat_req: ChatCompletionsReqGuarded): chat_req_dict = chat_req.dict() guard_to_apply = available_guards[chat_req.guard_to_apply] chat_req_dict.pop(\u0026quot;guard_to_apply\u0026quot;) if chat_req.stream: def stream_responses(): completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, chat_request=chat_req_dict, guard_to_apply=guard_to_apply) for result in completion_outcome: yield str(result) + \u0026quot; \u0026quot; return StreamingResponse(stream_responses(), media_type=\u0026quot;text/event-stream\u0026quot;) else: completion_outcome = call_llm_guarded(provider_base=LLM_PROVIDER_BASE, provider_key=LLM_API_TOKEN, chat_request=chat_req_dict, guard_to_apply=guard_to_apply) return completion_outcome#FIX THIS # Function to run the server in a background thread def run(): nest_asyncio.apply() uvicorn.run(app_guarded, host=\u0026quot;0.0.0.0\u0026quot;, port=8000) # Start the FastAPI server in a separate thread server_thread = threading.Thread(target=run, daemon=True) server_thread.start()  %%bash curl -X 'POST' \\ 'http://localhost:8000/ChatCompletionsReqGuarded' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Are python developers dumb idiotic and should they use rust \u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot;, \u0026quot;guard_to_apply\u0026quot;:\u0026quot;Profanity\u0026quot; }'  %%bash curl -X 'POST' \\ 'http://localhost:8000/ChatCompletionsReqGuarded' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \u0026quot;messages\u0026quot;:[ {\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Complete the below sentence. he is in id**t \u0026quot;} ], \u0026quot;stream\u0026quot;:true, \u0026quot;max_tokens\u0026quot;:50, \u0026quot;model\u0026quot;: \u0026quot;gpt-3.5-turbo\u0026quot;, \u0026quot;guard_to_apply\u0026quot;:\u0026quot;Profanity\u0026quot; }'   ","date":1740240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740240000,"objectID":"c0f36ed2262d4cf61a7002866c4cb7c3","permalink":"/tutorial/guardrails-ai/guardrail_serve/","publishdate":"2025-02-23T00:00:00+08:00","relpermalink":"/tutorial/guardrails-ai/guardrail_serve/","section":"tutorial","summary":"1. Set up environment 1.1 Install a Virtual env with all dependencies 1.1.1 UV Based Environment Creation  Running below cell requires uv to be installed on your machine. You can install from https://docs.astral.sh/uv/pip/environments/ If you dont want UV please use pip based install  %%bash uv venv guarded_llm_env source guarded_llm_env/bin/activate uv pip install ipykernel nbconvert uv pip install guardrails-ai==0.6.3 --prerelease allow uv pip install fastapi uvicorn nest-asyncio python -m ipykernel install --user --name=guarded_llm_env  1.","tags":null,"title":"Part 2: Deploying a Guarded Endpoint Using FastAPI","type":"docs"},{"authors":null,"categories":null,"content":"Sarathi is an LLM-based, simple command line interface-based coding assistant. ThIt ca write detailed commit messages and documentation for a python codebases here\n","date":1715011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715011200,"objectID":"d947aae5fcd6f616dd8d0f0591ebf730","permalink":"/opensource/sarathi/","publishdate":"2024-05-07T00:00:00+08:00","relpermalink":"/opensource/sarathi/","section":"opensource","summary":"A LLM Based CLI coding assistant","tags":["machine-learning","llm","git"],"title":"sarathi","type":"opensource"},{"authors":["Stefanus Agus Haryono","Hong Jin Kang","**Abhishek Sharma**","Asankhaya Sharma","Andrew Santosa","Ang Ming Yi","David Lo"],"categories":null,"content":"","date":1647619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647619200,"objectID":"a25f415706a533234675c5d7671ccac4","permalink":"/publication/xmtc/","publishdate":"2022-03-19T00:00:00+08:00","relpermalink":"/publication/xmtc/","section":"publication","summary":"Software engineers depend heavily on software libraries and have to update their dependencies once vulnerabilities are found in them. Software Composition Analysis (SCA) helps developers identify vulnerable libraries used by an application. A key challenge is the  dentification of libraries related to a given reported vulnerability in the National Vulnerability Database (NVD), which may not explicitly indicate the affected libraries.  Recently, researchers have tried to address the problem of identifying the libraries from an NVD report by treating it as an extreme multi-label learning (XML) problem, characterized by its large number of possible labels and severe data sparsity. As input, the NVD report is provided, and as output, a set of relevant libraries is returned. In this work, we evaluated multiple XML techniques and performed an analysis of different models proposed for XML classification. While previous work only evaluated a traditional XML technique, FastXML, we trained four other traditional XML models (DiSMEC, Parabel, Bonsai, ExtremeText) as well as two deep learning-based models (XML-CNN and LightXML). We compared the performance in both their effectiveness and the time cost of training and using the models for predictions. We find that other than DiSMEC and XML-CNN, recent XML models outperform the FastXML model by 3%–10% in terms of F1-scores on Top-k (k=1,2,3) predictions. Furthermore, we observe significant improvements in both the training and prediction time of these XML models, with Bonsai and Parabel model achieving 627x and 589x faster training time and 12x faster prediction time from the FastXML baseline. From a deeper analysis, we discuss the implications of our experimental results and highlight limitations that future work needs to address","tags":["ICPC","2022"],"title":"Automated Identification of Libraries from Vulnerability Data:Can We Do Better?","type":"publication"},{"authors":["**Abhishek Sharma**","Gede Artha Azriadi Prana","Anamika Sawhney","Nachiappan Nagappan","David Lo"],"categories":null,"content":"","date":1642093200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642093200,"objectID":"525ec1f0be3e194631737dea73bc2a85","permalink":"/publication/meetup/","publishdate":"2022-01-14T01:00:00+08:00","relpermalink":"/publication/meetup/","section":"publication","summary":"Software developers use a variety of social media channels and tools in order to keep themselves up to date, collaborate with other developers, and find projects to contribute to. Meetup is one of such social media used by software developers to organize community gatherings. We in this work investigate the dynamics of Meetup groups and events related to software development. Our work is different from previous work as we focus on the actual event and group data that was collected using Meetup API.In this work, we performed an empirical study of events and groups present on Meetup which are related to software development. First, we identified 6,327 Meetup groups related to software development and extracted 250,369 events organized by them. Then we took a sample of 452 events on which we performed open coding, based on which we were able to develop 9 categories of events (8 main categories +“Others”). Next, we did a popularity analysis of the categories of events and found that Talks by Domain Experts, Hands-on Sessions, and Open Discussions are the most popular categories of events organized by Meetup groups related to software development. Our findings show that more popular categories are those where developers can learn and gain knowledge. On doing a diversity analysis of   Meetup groups we found 20.46% of the members on average are female, and 20.34% of the actual event participants are female, which is a larger proportion as compared to numbers reported in previous studies on gender representation in software engineering communities. We also found evidence that the gender of Meetup group organizer affects gender distribution of group members and event participants. Finally, we also looked at some data on how COVID-19 has affected the Meetup activity and found that the event activity has dropped, but not stalled. A substantial number of events are now being organized virtually. The results and insights uncovered in our work can guide future studies related to software communities, groups, and diversity-related studies..","tags":["SANER","2022"],"title":"Analyzing Offline Social Engagements: AnEmpirical Study of Meetup Events Related to Software Development","type":"publication"},{"authors":["Giang Nguyen-Truong","Hong Jin Kang","David Lo","**Abhishek Sharma**","Andrew Santosa","Asankhaya Sharma","Ang Ming Yi"],"categories":null,"content":"","date":1642089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642089600,"objectID":"1ddfde26f91699375cd78e2f71c62df1","permalink":"/publication/vcur2/","publishdate":"2022-01-14T00:00:00+08:00","relpermalink":"/publication/vcur2/","section":"publication","summary":"Software projects today rely on many third-party libraries, and therefore, are exposed to vulnerabilities in these libraries. When a library vulnerability is fixed, users are notified and advised to upgrade to a new version of the library. However, not all vulnerabilities are publicly disclosed, and users may not be aware of vulnerabilities that may affect their applications. Due to the above challenges, there is a need for techniques which can identify and alert users to silent fixes in libraries; commits that fix bugs with security implications that are not officially disclosed. We propose a machine learning approach to automatically identify vulnerability-fixing commits. Existing techniques consider only data within a commit, such as its commit message, which does not always have sufficiently discriminative information. To address this limitation, our approach incorporates the rich source of information from issue trackers. When a commit does not link to an issue, we use a commit-issue link recovery technique to infer the potential missing link. Our experiments are promising; incorporating information from issue trackersboosts the performance of a vulnerability-fixing commit classifier, improving over the strongest baseline by 11.1% on the entire dataset, which includes commits that do not link to an issue. On a subset of the data in which all commits explicitly link to an issue, our approach improves over the baseline by 12.5%.","tags":["SANER","2022"],"title":"HERMES: Using Commit-Issue Linking to Detect Vulnerability-Fixing Commits","type":"publication"},{"authors":null,"categories":null,"content":"A mini Dockerized ML system with MLOPS capabilities (train/deploy). It can train/deploy models for recommendation/classification given user+stores click interaction data set and automatically deploy models to be served by a simple python web API end point. You can find the source code of the application here\n","date":1638806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638806400,"objectID":"c269b93d68c0316dd9e44a141cee5b43","permalink":"/opensource/mlops/","publishdate":"2021-12-07T00:00:00+08:00","relpermalink":"/opensource/mlops/","section":"opensource","summary":"A mini Dockerized ML system with MLOPS capabilities (train/deploy). It can train/deploy models for recommendation/classification ","tags":["machine-learning","ml-ops","tensorflow","docker"],"title":"mlopsdemo","type":"opensource"},{"authors":["Bowen Xu","Thong Hoang","**Abhishek Sharma**","Chengran Yang","Xin Xia","David Lo"],"categories":null,"content":"","date":1625500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625500800,"objectID":"7d19da295d2d832d57ecca977ff711bf","permalink":"/publication/post2vec/","publishdate":"2021-07-06T00:00:00+08:00","relpermalink":"/publication/post2vec/","section":"publication","summary":"Past studies have proposed solutions that analyze Stack Overflow content to help users find desired information or aid various downstream software engineering tasks. A common step performed by those solutions is to extract suitable representations of posts; typically, in the form of meaningful vectors. These vectors are then used for different tasks, for example, tag recommendation, relatedness prediction, and post classification. Intuitively, the quality of the vector representations of posts determines the effectiveness of the solutions in performing the respective tasks. In this work, to aid existing studies that analyze Stack Overflow posts, we propose a specialized deep learning architecture Post2Vec which extracts distributed representations of Stack Overflow posts. Post2Vec is aware of different types of content present in Stack Overflow posts, i.e., title, description, and code snippets, and integrates them seamlessly to learn post representations. Tags provided by Stack Overflow users that serve as a common vocabulary that captures the semantics of posts are used to guide Post2Vec in its task. To evaluate the quality of Post2Vec's deep learning architecture, we first investigate its end-to-end effectiveness in tag recommendation task. The results are compared to those of state-of-the-art tag recommendation approaches that also employ deep neural networks. We observe that Post2Vec achieves 15-25% improvement in terms of F1-score@5 at a lower computational cost. Moreover, to evaluate the value of representations learned by Post2Vec, we use them for three other tasks, i.e., relatedness prediction, post classification, and api recommendation. We demonstrate that the representations can be used to boost the effectiveness of state-of-the-art solutions for the two tasks by substantial margins (by 10%, 7%, and 10% in terms of F1-score, F1-score, and correctness, respectively). We release our replication package at https://github.com/Post2Vec/Post2Vec.","tags":["TSE","2021"],"title":"Post2Vec: Learning Distributed Representations of Stack Overflow Posts","type":"publication"},{"authors":["Gede Artha Azriadi Prana","**Abhishek Sharma**","Lwin Khin Shar","Darius Foo","Andrew E. Santosa","Asankhaya Sharma","David Lo"],"categories":null,"content":"","date":1618934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618934400,"objectID":"74f969f330df38c8e7d64f7da0eab212","permalink":"/publication/outofsight/","publishdate":"2021-04-21T00:00:00+08:00","relpermalink":"/publication/outofsight/","section":"publication","summary":"Software developers often use open-source libraries in their project to improve development speed. However, such libraries may contain security vulnerabilities, and this has resulted in several high-profile incidents in recent years. As usage of open-source libraries grows, understanding of these dependency vulnerabilities becomes increasingly important. In this work, we analyze vulnerabilities in open-source libraries used by 450 software projects written in Java, Python, and Ruby. Our goal is to examine types, distribution, severity, and persistence of the vulnerabilities, along with relationships between their prevalence and project as well as commit attributes. Our data is obtained by scanning versions of the sample projects after each commit made between November 1, 2017 and October 31, 2018 using an industrial software composition analysis tool, which provides information such as library names and versions, dependency types (direct or transitive), and known vulnerabilities. Among other findings, we found that project activity level, popularity, and developer experience do not translate into better or worse handling of dependency vulnerabilities. We also found “Denial of Service” and “Information Disclosure” types of vulnerabilities being common across the languages studied. Further, we found that most dependency vulnerabilities persist throughout the observation period (mean of 78.4%, 97.7%, and 66.4% for publicly-known vulnerabilities in our Java, Python, and Ruby datasets respectively), and the resolved ones take 3-5 months to fix. Our results highlight the importance of managing the number of dependencies and performing timely updates, and indicate some areas that can be prioritized to improve security in wide range of projects, such as prevention and mitigation of Denial-of-Service attacks.","tags":["EMSE","2021"],"title":"Out of sight, out of mind? How vulnerable dependencies affect open-source projects","type":"publication"},{"authors":["Chen Yang","Andrew Santosa","Ang Ming Yi","**Abhishek Sharma**","Asankhaya Sharma","David Lo"],"categories":null,"content":"","date":1593100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593100800,"objectID":"480e3e6acb42cc1232f6c1a6fded3250","permalink":"/publication/vulncur/","publishdate":"2020-06-26T00:00:00+08:00","relpermalink":"/publication/vulncur/","section":"publication","summary":"Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models’ quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59%  maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources","tags":["MSR","2020","Awards"],"title":"A Machine Learning Approach for Vulnerability Curation","type":"publication"},{"authors":[" Agus Sulistya","Gede Artha Azriadi Prana","**Abhishek Sharma**","David Lo","Christoph Treude"],"categories":null,"content":"","date":1569513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569513600,"objectID":"aa65e7d7d555d6be40563cebd90fb299","permalink":"/publication/sieve/","publishdate":"2019-09-27T00:00:00+08:00","relpermalink":"/publication/sieve/","section":"publication","summary":" Software developers have benefited from various sources of knowledge such as forums, question-and-answer sites, and social media platforms to help them in various tasks. Extracting software-related knowledge from different platforms involves many challenges. In this paper, we propose an approach to improve the effectiveness of knowledge extraction tasks by performing crossplatform analysis. Our approach is based on transfer representation learning and word embeddings, leveraging information extracted from a source platform which contains rich domain-related content. The information extracted is then used to solve tasks in another platform (considered as target platform) with less domain-related contents. We first build a word embeddings model as a representation learned from the source platform, and use the model to improve the performance of knowledge extraction tasks in the target platform. We experiment with Software Engineering Stack Exchange and Stack Overflow as source platforms, and two different target platforms, i.e., Twitter and YouTube. Our experiments show that our approach improves performance of existing work for the tasks of identifying software-related tweets and helpful YouTube comments.","tags":["EMSE","2020"],"title":"SIEVE: Helping Developers Sift Wheat from Chaff via Cross-Platform Analysis","type":"publication"},{"authors":["**Abhishek Sharma**","Yuan Tian","Agus Sulistya","Dinusha Wijedasa","David Lo"],"categories":null,"content":"","date":1537977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537977600,"objectID":"bb073f3e71533b29a705f4a5b2027a85","permalink":"/publication/software-experts/","publishdate":"2018-09-27T00:00:00+08:00","relpermalink":"/publication/software-experts/","section":"publication","summary":"With the advent of social media, developers are increasingly using it in their software development activities. Twitter is also one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape”. Unfortunately, due to the general purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets. To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domainrelated tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over one month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-measure by at least 7.63%.","tags":["TOSEM","2018"],"title":"Recommending Who to Follow in the Software Engineering Twitter Space","type":"publication"},{"authors":null,"categories":null,"content":"","date":1536422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536422400,"objectID":"75d35f85cd14a74dc235081d73a2355a","permalink":"/tutorial/guardrails-ai/guardrail_validator/","publishdate":"2018-09-09T00:00:00+08:00","relpermalink":"/tutorial/guardrails-ai/guardrail_validator/","section":"tutorial","summary":"","tags":null,"title":"Part 3: Adding a Custom Validator to Guardrails-AI","type":"docs"},{"authors":["Yuan Tian","Ferdian Thung","**Abhishek Sharma**","David Lo"],"categories":null,"content":"","date":1510329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510329600,"objectID":"b98029c3e61d20a20ebf03fe07928a45","permalink":"/publication/apibot/","publishdate":"2017-11-11T00:00:00+08:00","relpermalink":"/publication/apibot/","section":"publication","summary":"As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.","tags":["ASE","2017"],"title":"APIBot: Question Answering Bot for API documentation","type":"publication"},{"authors":["Xin Xia","David Lo","Lingfeng Bao","**Abhishek Sharma**","Shanping Li"],"categories":null,"content":"","date":1505577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505577600,"objectID":"9e17c062d798d9af823503b4d234e2c9","permalink":"/publication/personality/","publishdate":"2017-09-17T00:00:00+08:00","relpermalink":"/publication/personality/","section":"publication","summary":"A software project is typically completed as a result of a collective effort done by individuals of different personalities. Personality reflects differences among people in behaviour patterns, communication, cognition and emotion. It often impacts relationships and collaborative work, and software engineering teamwork is no exception. Some personalities are more likely to click while others to clash. A number of studies have investigated the relationship between personality and collaborative work success. However, most of them are done in a laboratory setting, do not involve professionals, or consider non software engineering tasks. Additionally, they only answer a limited set of questions, and many other questions remain open.To enrich the existing body of work, we study professionals working on real software projects, answering a new set of research questions that assess linkages between project manager personality and team personality composition and project success. In particular, our study investigates 28 recently completed software projects, which contain a total of 346 professionals, in 2 large IT companies. We asked project members to do a DISC (Dominance, Influence, Steadiness, and Compliant) personality test, and correlated the test outcomes with project success scores measured in six different dimensions. The scores were given by managers of three office as part of their regular day-to-day work. Our results show that project teams with dominant managers, along with those with more influential members and less dominant members, have higher success scores. This work provides new insights to construct a personality matching strategy that can contribute to building an effective project team.","tags":["ICSME","2017"],"title":"Personality and Project Success: Insights from a Large-Scale Study with Professionals","type":"publication"},{"authors":["**Abhishek Sharma**","Ferdian Thung","Pavneet Singh Kochhar","Agus Sulistya","David Lo"],"categories":null,"content":"","date":1497456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497456000,"objectID":"4cc177b9cf67066aa950adcd518ade66","permalink":"/publication/cataloging/","publishdate":"2017-06-15T00:00:00+08:00","relpermalink":"/publication/cataloging/","section":"publication","summary":"GitHub is one of the largest and most popular repository hosting service today, having about 14 million users and more than 54 million repositories as of March 2017. This makes it an excellent platform to find projects that developers are interested in exploring. GitHub showcases its most popular projects by cataloging them manually into categories such as DevOps tools, web application frameworks, and game engines. We propose that such cataloging should not be limited only to popular projects. We explore the possibility of developing such cataloging system by automatically extracting functionality descriptive text segments from readme files of GitHub repositories. These descriptions are then input to LDA-GA, a state-of-the-art topic modeling algorithm, to identify categories. Our preliminary experiments demonstrate that additional meaningful categories which complement existing GitHub categories can be inferred. Moreover, for inferred categories that match GitHub categories, our approach can identify additional projects belonging to them. Our experimental results establish a promising direction in realizing automatic cataloging system for GitHub.","tags":["EASE","2017"],"title":"Cataloging github repositories","type":"publication"},{"authors":["**Abhishek Sharma**","Yuan Tian","Agus Sulistya","David Lo","Aiko Fallas Yamashita"],"categories":null,"content":"","date":1487692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487692800,"objectID":"fdb2f5807b5d3fe71552e501cf3f226b","permalink":"/publication/harnessingurl/","publishdate":"2017-02-22T00:00:00+08:00","relpermalink":"/publication/harnessingurl/","section":"publication","summary":"Developers often rely on various online resources, such as blogs, to keep themselves up-to-date with the fast pace at which software technologies are evolving. Singer et al. found that developers tend to use channels such as Twitter to keep themselves updated and support learning, often in an undirected or serendipitous way, coming across things that they may not apply presently, but which should be helpful in supporting their developer activities in future. However, identifying relevant and useful articles among the millions of pieces of information shared on Twitter is a non-trivial task. In this work to support serendipitous discovery of relevant and informative resources to support developer learning, we propose an unsupervised and a supervised approach to find and rank URLs (which point to web resources) harvested from Twitter based on their informativeness and relevance to a domain of interest. We propose 14 features to characterize each URL by considering contents of webpage pointed by it, contents and popularity of tweets mentioning it, and the popularity of users who shared the URL on Twitter. The results of our experiments on tweets generated by a set of 85,171 users over a one-month period highlight that our proposed unsupervised and supervised approaches can achieve a reasonably high Normalized Discounted Cumulative Gain (NDCG) score of 0.719 and 0.832 respectively.","tags":["SANER","2017","Awards"],"title":"Harnessing Twitter to support serendipitous learning of developers","type":"publication"},{"authors":["Agus Sulistya","**Abhishek Sharma**","David Lo"],"categories":null,"content":"","date":1476806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476806400,"objectID":"344ed650699a3fa642ae1cf639d8954a","permalink":"/publication/spiteful/","publishdate":"2016-10-19T00:00:00+08:00","relpermalink":"/publication/spiteful/","section":"publication","summary":"Social media provides a convenient way for customers to express their feedback to companies. Identifying different types of customers based on their feedback behavior can help companies to maintain their customers. In this paper, we use a machine learning approach to predict a customer’s feedback behavior based on her first feedback tweet. First, we identify a few categories of customers based on their feedback frequency and the sentiment of the feedback. We identify three main categories: spiteful, one-off, and kind. Next, we build a model to predict the category of a customer given her first feedback. We use profile and content features extracted from Twitter. We experiment with different algorithms to create a prediction model. Our study shows that the model is able to predict different types of customers and perform better than a baseline approach in terms of precision, recall, and F-measure.","tags":["SocInfo","2016"],"title":"Spiteful, One-Off, and Kind: Predicting Customer Feedback Behavior on Twitter","type":"publication"},{"authors":null,"categories":null,"content":"Our paper Recommending Who to Follow in the Software Engineering Twitter Space has been selected for publication in the journal Transactions on Software Engineering and Methodology (TOSEM).\n","date":1475251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540828800,"objectID":"dd8f117b63128f4de787e7d785e5bd9c","permalink":"/post/news/","publishdate":"2016-10-01T00:00:00+08:00","relpermalink":"/post/news/","section":"post","summary":"Paper accepted for publication in TOSEM","tags":["news"],"title":"Latest News","type":"post"},{"authors":["Doyen Sahoo","**Abhishek Sharma**","Steven C.H. Hoi","Peilin Zhao"],"categories":null,"content":"","date":1462377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462377600,"objectID":"aff50a3baf124d16f1d3d53dc6f51c7c","permalink":"/publication/temporalkernel/","publishdate":"2016-05-05T00:00:00+08:00","relpermalink":"/publication/temporalkernel/","section":"publication","summary":"Detecting temporal patterns is one of the most prevalent challenges while mining data. Often, timestamps or information about when certain instances or events occurred can provide us with critical information to recognize temporal patterns. Unfortunately, most existing techniques are not able to fully extract useful temporal information based on the time (especially at different resolutions of time). They miss out on 3 crucial factors: (i) they do not distinguish between timestamp features (which have cyclical or periodic properties) and ordinary features; (ii) they are not able to detect patterns exhibited at different resolutions of time (e.g. different patterns at the annual level, and at the monthly level); and (iii) they are not able to relate different features (e.g. multimodal features) of instances with different temporal properties (e.g. while predicting stock prices, stock fundamentals may have annual patterns, and at the same time factors like peer stock prices and global markets may exhibit daily patterns). To solve these issues, we offer a novel multiple-kernel learning view and develop Temporal Kernel Descriptors which utilize Kernel functions to comprehensively detect temporal patterns by deriving relationship of instances with the time features. We automatically learn the optimal kernel function, and hence the optimal temporal similarity between two instances. We formulate the optimization as a Multiple Kernel Learning (MKL) problem. We empirically evaluate its performance by solving the optimization using Online MKL","tags":["SDM","2016"],"title":"Temporal kernel descriptors for learning with time-sensitive patterns?","type":"publication"},{"authors":["**Abhishek Sharma**","Yuan Tian","David Lo"],"categories":null,"content":"","date":1443628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443628800,"objectID":"8214e75c1e8b295491f8128501540006","permalink":"/publication/twitterevents/","publishdate":"2015-10-01T00:00:00+08:00","relpermalink":"/publication/twitterevents/","section":"publication","summary":"Twitter is a popular means to disseminate information and currently more than 300 million people are using it actively. Software engineers are no exception; Singer et al. have shown that many developers use Twitter to stay current with recent technological trends. At various time points, many users are posting microblogs (i.e., tweets) about the same topic in Twitter. We refer to this reasonably large set of topically-coherent microblogs in the Twitter space made at a particular point in time as an event. In this work, we perform an exploratory study on software engineering related events in Twitter. We collect a large set of Twitter messages over a period of 8 months that are made by 79,768 Twitter users and filter them by five programming language keywords. We then run a state-of-the-art Twitter event detection algorithm borrowed from the Natural Language Processing (NLP) domain. Next, using the open coding procedure, we manually analyze 1,000 events that are identified by the NLP tool, and create eleven categories of events (10 main categories + “others”). We find that external resource sharing, technical discussion, and software product updates are the “hottest” categories. These findings shed light on hot topics in Twitter that are interesting to many people and they provide guidance to future Twitter analytics studies that develop automated solutions to help users find fresh, relevant, and interesting pieces of information from Twitter stream to keep developers up-to-date with recent trends.","tags":["ICSME","2015"],"title":"What's Hot in Software Engineering Twitter Space?","type":"publication"},{"authors":["**Abhishek Sharma**","Yuan Tian","David Lo"],"categories":null,"content":"","date":1426348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426348800,"objectID":"e0ebf5aaa078d3acb84989d6f1b00406","permalink":"/publication/nirmal/","publishdate":"2015-03-15T00:00:00+08:00","relpermalink":"/publication/nirmal/","section":"publication","summary":"Twitter is one of the most widely used social media platforms today. It enables users to share and view short 140-character messages called “tweets”. About 284 million active users generate close to 500 million tweets per day. Such rapid generation of user generated content in large magnitudes results in the problem of information overload. Users who are interested in information related to a particular domain have limited means to filter out irrelevant tweets and tend to get lost in the huge amount of data they encounter. A recent study by Singer et al. found that software developers use Twitter to stay aware of industry trends, to learn from others, and to network with other developers. However, Singer et al. also reported that developers often find Twitter streams to contain too much noise which is a barrier to the adoption of Twitter. In this paper, to help developers cope with noise, we propose a novel approach named NIRMAL, which automatically identifies software relevant tweets from a collection or stream of tweets. Our approach is based on language modeling which learns a statistical model based on a training corpus (i.e., set of documents). We make use of a subset of posts from StackOverflow, a programming question and answer site, as a training corpus to learn a language model. A corpus of tweets was then used to test the effectiveness of the trained language model. The tweets were sorted based on the rank the model assigned to each of the individual tweets. The top 200 tweets were then manually analyzed to verify whether they are software related or not, and then an accuracy score was calculated. The results show that decent accuracy scores can be achieved by various variants of NIRMAL, which indicates that NIRMAL can effectively identify software related tweets from a huge corpus of tweets.","tags":["SANER","2015"],"title":"Nirmal: Automatic identification of software relevant tweets leveraging language model","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd74d3ab6c31894031912915faa45e6c","permalink":"/project/ease/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/ease/","section":"project","summary":"","tags":null,"title":"","type":"project"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]